{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Chinese Beijing Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Chinese Chinese Shanghai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Chinese Macao</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tokyo Japan Chinese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y                     words\n",
       "0  1   Chinese Beijing Chinese\n",
       "1  1  Chinese Chinese Shanghai\n",
       "2  1             Chinese Macao\n",
       "3  0       Tokyo Japan Chinese"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer #计算词频的包\n",
    "path = 'datas/bayes_xinxi.txt'\n",
    "df = pd.read_csv(path)\n",
    "df\n",
    "tfCoder = CountVectorizer(token_pattern='[a-zA-Z|\\u4e00-\\u9fa5]+')\n",
    "tfCoder.fit(df['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Chinese Beijing Chinese\n",
       "1    Chinese Chinese Shanghai\n",
       "2               Chinese Macao\n",
       "3         Tokyo Japan Chinese\n",
       "Name: words, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df = pd.read_csv(\"datas/bayes_xinxi.txt\")\n",
    "tfCoder = CountVectorizer(token_pattern=\"[a-zA-Z|\\u4e00-\\u9fa5]+\")\n",
    "X = df[\"words\"]\n",
    "Y = df[\"Y\"]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n",
      "[[1 2 0 0 0 0]\n",
      " [0 2 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 1 0 0 1]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "X = tfCoder.fit_transform(X)\n",
    "print(tfCoder.get_feature_names())\n",
    "print(X.toarray())\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X,Y)\n",
    "T = ['Chinese Chinese Chinese Tokyo Japan']\n",
    "print(model.predict(tfCoder.transform(T).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n",
      "feature_prob [[0.11111111 0.22222222 0.22222222 0.11111111 0.11111111 0.22222222]\n",
      " [0.14285714 0.42857143 0.07142857 0.14285714 0.14285714 0.07142857]]\n",
      "-----------进行测试-----------\n",
      "X [[0 3 1 0 0 1]]\n",
      "past [[1.         0.01097394 0.22222222 1.         1.         0.22222222]\n",
      " [1.         0.0787172  0.07142857 1.         1.         0.07142857]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "df = pd.read_csv(\"datas/bayes_xinxi.txt\")  #读取数据\n",
    "tfCoder = CountVectorizer(token_pattern=\"[a-zA-Z|\\u4e00-\\u9fa5]+\")#TF模型\n",
    "lam = 1\n",
    "tfCoder.fit(df[\"words\"])#训练TF模型，只需要fit一次，确定词表\n",
    "names_feature = tfCoder.get_feature_names()  #获取词语名  \n",
    "\n",
    "print(names_feature)\n",
    "Y =  list(set(df[\"Y\"]))#获取类别种类的list\n",
    "class_prior = [] #用于存储先验概率\n",
    "feature_prob = [] #用于存储条件概率\n",
    "for y in Y: #遍历每个类别\n",
    "    df2 = df[df[\"Y\"]==y]#获取每个类别下的DF\n",
    "\n",
    "    prior = df2.shape[0]/df.shape[0] #计算先验概率\n",
    "    class_prior.append(prior)#将先验概率加入集合\n",
    "    \n",
    "    a = tfCoder.transform(df2[\"words\"]).toarray()\n",
    "    prob = (a.sum(axis=0) + lam)/(a.sum()+len(names_feature))#计算条件概率\n",
    "    feature_prob.append(prob)#将条件概率加入集合\n",
    "    \n",
    "feature_prob = np.array(feature_prob)\n",
    "print(\"feature_prob\",feature_prob)\n",
    "print(\"-----------进行测试-----------\")\n",
    "X = ['Chinese Chinese Chinese Tokyo Japan'] #训练数据\n",
    "X = tfCoder.transform(X).toarray()  #将训练数据转为array类型\n",
    "print(\"X\",X)\n",
    "past = feature_prob**X #求后验概率\n",
    "print(\"past\",past)\n",
    "#输出概率最大的类别\n",
    "Y[np.argmax(np.exp(np.sum(np.log(past),axis=1))*class_prior)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '恨', '我', '爱']\n",
      "[[1 0 1 1]\n",
      " [2 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X = ['我 爱 你','我 恨 你 恨 你']\n",
    "y = [0,1]\n",
    "countCoder = CountVectorizer(token_pattern=\"[a-zA-Z|\\u4e00-\\u9fa5]+\")\n",
    "X = countCoder.fit_transform(X)\n",
    "print(countCoder.get_feature_names())  \n",
    "print(X.toarray())\n",
    "# ['你', '恨', '我', '爱']\n",
    "# [[1 0 1 1]\n",
    "#  [2 2 1 0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1]\n",
      "[0 0 1 0 1 1]\n",
      "[1 1 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886\n",
      "0.8528448628735161\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(\"datas/bayes_wenben.txt\",header=None)\n",
    "\n",
    "\n",
    "X = df[1]\n",
    "Y = df[0]\n",
    "tfCoder = CountVectorizer(token_pattern=\"[a-zA-Z|\\u4e00-\\u9fa5]+\")\n",
    "X = tfCoder.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "print(y_test.shape[0])\n",
    "print(sum(model.predict(X_test)==y_test.values)/y_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
